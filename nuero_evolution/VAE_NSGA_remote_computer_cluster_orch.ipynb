{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f7fc35d-3eee-4b5c-9182-f89e05c49b50",
   "metadata": {},
   "source": [
    "# Workflow of VAE-assisted NSGA2 LDSE using Remote Compute Orchastration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6283d4-4c75-4c38-ae41-1e391b27b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NSGA2 imports\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.core.problem import Problem\n",
    "from pymoo.operators.crossover.pntx import PointCrossover\n",
    "from pymoo.operators.crossover.sbx import SBX\n",
    "from pymoo.operators.mutation.pm import PolynomialMutation\n",
    "from pymoo.operators.sampling.rnd import FloatRandomSampling\n",
    "from pymoo.termination import get_termination\n",
    "from pymoo.core.callback import Callback\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import pickle\n",
    "\n",
    "\n",
    "# ================ MODIFIED CONFIG ===================\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_CLASSES = 10\n",
    "INPUT_DIM = 4096\n",
    "NUM_CATEGORIES = 10\n",
    "SAVE_DIR = 'categorical_vae_models'\n",
    "LATENT_DIM = 64   # We're using the 64-dimensional model\n",
    "\n",
    "# NSGA2 Configuration - MODIFIED\n",
    "GENERATIONS = 3\n",
    "POPULATION = 14  \n",
    "THREADS = 14    \n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3528c5-7592-43e3-9c57-22c8e837a42e",
   "metadata": {},
   "source": [
    "# Remote Servers Orchastration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a6d448-2583-445d-ab4f-4bd96b08c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds SCP logic to send array_config_X.dat files before job starts\n",
    "# Suppresses known_hosts warning without touching stderr\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "from datetime import timedelta\n",
    "\n",
    "# -------- CONFIG SECTION -------- #\n",
    "remote_configs = {\n",
    "    \"cadencea12@172.16.121.82\": {\"password\": \"caduser@123\", \"subdir\": [\"array_config_1\", \"array_config_2\"]},\n",
    "    \"cadencea1@172.16.121.72\": {\"password\": \"caduser@123\", \"subdir\": [\"array_config_5\", \"array_config_6\"]},\n",
    "    \"cadencea8@172.16.121.78\": {\"password\": \"caduser@123\", \"subdir\": [\"array_config_7\", \"array_config_8\"]},\n",
    "    \"cadencea10@172.16.121.80\": {\"password\": \"caduser@123\", \"subdir\": [\"array_config_9\", \"array_config_10\"]},\n",
    "    \"cadencea15@172.16.121.6\": {\"password\": \"caduser@123\", \"subdir\": [\"array_config_11\", \"array_config_12\"]},\n",
    "    \"cadencea14@172.16.121.84\": {\"password\": \"caduser@123\", \"subdir\": [\"array_config_13\", \"array_config_14\"]},\n",
    "    \"imt2022556_nishit@172.16.121.37\": {\"password\": \"$@Rl@1234\", \"subdir\": [\"array_config_3\", \"array_config_4\"]},\n",
    "}\n",
    "\n",
    "def run_remote_compute_cluster_automation(generation_idx):\n",
    "    config_mapping = {}\n",
    "    for user_host, config in remote_configs.items():\n",
    "        for subdir in config[\"subdir\"]:\n",
    "            config_num = int(subdir.split('_')[-1])\n",
    "            config_mapping[config_num] = (user_host, subdir)\n",
    "\n",
    "    log_dir_base = f\"./gen/{generation_idx}/logs\"\n",
    "    csv_dir = os.path.join(log_dir_base, \"csv_files\")\n",
    "    vitals_dir = os.path.join(log_dir_base, \"vitals\")\n",
    "\n",
    "    os.makedirs(log_dir_base, exist_ok=True)\n",
    "    os.makedirs(csv_dir, exist_ok=True)\n",
    "    os.makedirs(vitals_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # vitals_thread = threading.Thread(target=monitor_vitals, daemon=True)\n",
    "    # vitals_thread.start()\n",
    "\n",
    "    # -------- SCP CONFIG FILES BEFORE JOB START -------- #\n",
    "    for config_num in range(1, 15):\n",
    "        user_host, subdir = config_mapping[config_num]\n",
    "        remote_user, remote_host = user_host.split(\"@\")\n",
    "        password = remote_configs[user_host][\"password\"]\n",
    "\n",
    "        local_dat = f\"./gen/{generation_idx}/{subdir}.dat\"\n",
    "        remote_path = f\"/home/{remote_user}/Documents/strassen/{subdir}/{subdir}.dat\"\n",
    "\n",
    "        if os.path.exists(local_dat):\n",
    "            print(f\"üì§ Sending {local_dat} ‚Üí {remote_user}@{remote_host}:{remote_path}\")\n",
    "            scp_cmd = [\n",
    "                \"sshpass\", \"-p\", password,\n",
    "                \"scp\",\n",
    "                \"-o\", \"StrictHostKeyChecking=no\",\n",
    "                \"-o\", \"UserKnownHostsFile=/dev/null\",\n",
    "                local_dat,\n",
    "                f\"{remote_user}@{remote_host}:{remote_path}\"\n",
    "            ]\n",
    "            result = subprocess.run(scp_cmd)\n",
    "            if result.returncode == 0:\n",
    "                print(\"‚úÖ File sent successfully.\")\n",
    "            else:\n",
    "                print(\"‚ùå SCP failed.\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Missing local file: {local_dat}\")\n",
    "\n",
    "    # -------- JOB LAUNCH SECTION -------- #\n",
    "    processes = []\n",
    "\n",
    "    for config_num in range(1, 15):\n",
    "        user_host, subdir = config_mapping[config_num]\n",
    "        remote_user, remote_host = user_host.split(\"@\")\n",
    "        remote_password = remote_configs[user_host][\"password\"]\n",
    "\n",
    "        remote_path = f\"/home/{remote_user}/Documents/strassen/{subdir}\"\n",
    "        log_file = os.path.join(log_dir_base, f\"{config_num}.log\")\n",
    "\n",
    "        remote_command = f\"\"\"\n",
    "            cd {remote_path} && \\\n",
    "            python3  approx_mult_modules_automate.py && \\\n",
    "            python3 modify_rtl.py && \\\n",
    "            csh -c \"source cshrc && genus -legacy_ui -files script.tcl && exit\" && \\\n",
    "            python3 get_metrics.py\n",
    "        \"\"\"\n",
    "\n",
    "        full_command = [\n",
    "            \"sshpass\", \"-p\", remote_password,\n",
    "            \"ssh\", \"-tt\",\n",
    "            \"-o\", \"StrictHostKeyChecking=no\",\n",
    "            \"-o\", \"UserKnownHostsFile=/dev/null\",\n",
    "            f\"{remote_user}@{remote_host}\",\n",
    "            remote_command\n",
    "        ]\n",
    "\n",
    "        print(f\"üöÄ Launching job: Config {config_num} on {user_host} in {subdir} ‚Üí {log_file}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        f = open(log_file, \"w\")\n",
    "        p = subprocess.Popen(full_command, stdout=f, stderr=subprocess.STDOUT)\n",
    "\n",
    "        processes.append({\n",
    "            \"process\": p,\n",
    "            \"stdout\": f,\n",
    "            \"config_num\": config_num,\n",
    "            \"user_host\": user_host,\n",
    "            \"subdir\": subdir,\n",
    "            \"log_file\": log_file,\n",
    "            \"start_time\": start_time\n",
    "        })\n",
    "\n",
    "    # -------- WAIT & SCP METRICS SECTION -------- #\n",
    "    for job in processes:\n",
    "        proc = job[\"process\"]\n",
    "        proc.wait()\n",
    "        end_time = time.time()\n",
    "\n",
    "        duration = timedelta(seconds=(end_time - job[\"start_time\"]))\n",
    "        job[\"stdout\"].write(f\"\\n\\n‚úÖ Time taken: {duration}\\n\")\n",
    "        job[\"stdout\"].close()\n",
    "\n",
    "        print(f\"‚úÖ Completed: Config {job['config_num']} on {job['user_host']} [{job['subdir']}]\")\n",
    "        print(f\"üïí Time taken: {duration}\")\n",
    "        print(f\"üìÑ Log saved: {job['log_file']}\")\n",
    "\n",
    "        remote_user, remote_host = job[\"user_host\"].split(\"@\")\n",
    "        remote_password = remote_configs[job[\"user_host\"]][\"password\"]\n",
    "        remote_csv_path = f\"/home/{remote_user}/Documents/strassen/{job['subdir']}/metrics.csv\"\n",
    "        local_csv = os.path.join(csv_dir, f\"{job['config_num']}.csv\")\n",
    "\n",
    "        print(f\"üì• Fetching metrics.csv from Config {job['config_num']} ‚Üí {local_csv}\")\n",
    "\n",
    "        scp_cmd = [\n",
    "            \"sshpass\", \"-p\", remote_password,\n",
    "            \"scp\",\n",
    "            \"-o\", \"StrictHostKeyChecking=no\",\n",
    "            \"-o\", \"UserKnownHostsFile=/dev/null\",\n",
    "            f\"{remote_user}@{remote_host}:{remote_csv_path}\",\n",
    "            local_csv\n",
    "        ]\n",
    "        result = subprocess.run(scp_cmd)\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Metrics saved to: {local_csv}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to fetch metrics for Config {job['config_num']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aff2f1c-90bc-4280-a020-279575f20ad4",
   "metadata": {},
   "source": [
    "# VAE Decoder Envokation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ead43c-33fa-4cb6-bbc6-9416ec650f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ MODEL DEFINITIONS ===================\n",
    "class CategoricalEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_categories):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_categories = num_categories\n",
    "        \n",
    "        base_hidden = max(512, latent_dim * 4)\n",
    "        mid_hidden = max(256, latent_dim * 2)\n",
    "        \n",
    "        self.input_layer = nn.Linear(INPUT_DIM * NUM_CLASSES, base_hidden)\n",
    "        \n",
    "        num_res_blocks = max(2, min(4, latent_dim // 64))\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            self._make_res_block(base_hidden, base_hidden) for _ in range(num_res_blocks)\n",
    "        ])\n",
    "        \n",
    "        bottleneck_layers = []\n",
    "        current_dim = base_hidden\n",
    "        target_dim = mid_hidden\n",
    "        \n",
    "        if latent_dim >= 256:\n",
    "            bottleneck_layers.extend([\n",
    "                nn.Linear(current_dim, current_dim // 2),\n",
    "                nn.LayerNorm(current_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "            current_dim = current_dim // 2\n",
    "        \n",
    "        bottleneck_layers.extend([\n",
    "            nn.Linear(current_dim, target_dim),\n",
    "            nn.LayerNorm(target_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        ])\n",
    "        \n",
    "        self.bottleneck = nn.Sequential(*bottleneck_layers)\n",
    "        self.fc_out = nn.Linear(target_dim, latent_dim * num_categories)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _make_res_block(self, in_dim, out_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.LayerNorm(out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "            nn.LayerNorm(out_dim)\n",
    "        )\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        \n",
    "        for res_block in self.res_blocks:\n",
    "            residual = x\n",
    "            x = res_block(x)\n",
    "            x = F.relu(x + residual)\n",
    "        \n",
    "        x = self.bottleneck(x)\n",
    "        logits = self.fc_out(x).view(-1, self.latent_dim, self.num_categories)\n",
    "        \n",
    "        if self.training:\n",
    "            noise = torch.randn_like(logits) * 0.01\n",
    "            logits = logits + noise\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class CategoricalDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_categories):\n",
    "        super().__init__()\n",
    "        \n",
    "        min_hidden = max(256, latent_dim)\n",
    "        mid_hidden = max(512, latent_dim * 2)\n",
    "        max_hidden = max(1024, latent_dim * 3)\n",
    "        \n",
    "        layers = []\n",
    "        current_dim = latent_dim * num_categories\n",
    "        \n",
    "        layers.extend([\n",
    "            nn.Linear(current_dim, min_hidden),\n",
    "            nn.LayerNorm(min_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        ])\n",
    "        current_dim = min_hidden\n",
    "        \n",
    "        if latent_dim >= 128:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, mid_hidden),\n",
    "                nn.LayerNorm(mid_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            current_dim = mid_hidden\n",
    "        \n",
    "        if latent_dim >= 256:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, max_hidden),\n",
    "                nn.LayerNorm(max_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "            current_dim = max_hidden\n",
    "        \n",
    "        layers.append(nn.Linear(current_dim, INPUT_DIM * NUM_CLASSES))\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        z_flat = z.view(z.size(0), -1)\n",
    "        x_recon = self.fc(z_flat)\n",
    "        return x_recon.view(-1, INPUT_DIM, NUM_CLASSES)\n",
    "\n",
    "# ================ VAE UTILITIES ===================\n",
    "def load_model_fixed(latent_dim, model_path=None):\n",
    "    \"\"\"Load a trained model with fixed weights_only=False\"\"\"\n",
    "    if model_path is None:\n",
    "        model_path = f\"{SAVE_DIR}/best_model_latent_{latent_dim}.pt\"\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=DEVICE, weights_only=False)\n",
    "    \n",
    "    encoder = CategoricalEncoder(latent_dim, NUM_CATEGORIES).to(DEVICE)\n",
    "    decoder = CategoricalDecoder(latent_dim, NUM_CATEGORIES).to(DEVICE)\n",
    "    \n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    print(f\"‚úÖ Successfully loaded model with latent_dim={latent_dim}\")\n",
    "    return encoder, decoder, checkpoint\n",
    "\n",
    "def gumbel_softmax_with_controls(logits, tau, hard=False, dim=-1):\n",
    "    \"\"\"Improved Gumbel-Softmax with better numerical stability\"\"\"\n",
    "    if logits.requires_grad:\n",
    "        gumbels = -torch.empty_like(logits).exponential_().log()\n",
    "        gumbels = (logits + gumbels) / tau\n",
    "    else:\n",
    "        gumbels = logits / tau\n",
    "    \n",
    "    y_soft = F.softmax(gumbels, dim=dim)\n",
    "    \n",
    "    if hard:\n",
    "        index = y_soft.max(dim=dim, keepdim=True)[1]\n",
    "        y_hard = torch.zeros_like(logits).scatter_(dim, index, 1.0)\n",
    "        ret = y_hard - y_soft.detach() + y_soft\n",
    "    else:\n",
    "        ret = y_soft\n",
    "    \n",
    "    return ret\n",
    "\n",
    "# ================ LATENT SPACE ANALYSIS ===================\n",
    "class LatentSpaceBounds:\n",
    "    \"\"\"Analyze and determine bounds for the latent space\"\"\"\n",
    "    def __init__(self, encoder, decoder, num_samples=1000):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.bounds = None\n",
    "        self._analyze_bounds(num_samples)\n",
    "    \n",
    "    def _analyze_bounds(self, num_samples):\n",
    "        \"\"\"Analyze the latent space to determine reasonable bounds\"\"\"\n",
    "        print(f\"üîç Analyzing latent space bounds with {num_samples} samples...\")\n",
    "        \n",
    "        # Generate diverse training data\n",
    "        train_data = self._generate_diverse_data(num_samples)\n",
    "        \n",
    "        # Encode to get latent representations\n",
    "        latent_samples = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(train_data), 100):  # Process in batches\n",
    "                batch = train_data[i:i+100]\n",
    "                batch_tensor = torch.tensor(batch, dtype=torch.float32).to(DEVICE)\n",
    "                x = batch_tensor.view(-1, INPUT_DIM * NUM_CLASSES)\n",
    "                \n",
    "                logits = self.encoder(x)\n",
    "                z = gumbel_softmax_with_controls(logits, tau=0.5, hard=False)\n",
    "                latent_samples.append(z.cpu().numpy())\n",
    "        \n",
    "        latent_samples = np.concatenate(latent_samples, axis=0)\n",
    "        print(f\"üìä Latent samples shape: {latent_samples.shape}\")\n",
    "        \n",
    "        # Analyze bounds for each dimension and category\n",
    "        self.bounds = {}\n",
    "        self.bounds['min'] = np.min(latent_samples, axis=0)  # Shape: (64, 10)\n",
    "        self.bounds['max'] = np.max(latent_samples, axis=0)  # Shape: (64, 10)\n",
    "        self.bounds['mean'] = np.mean(latent_samples, axis=0)\n",
    "        self.bounds['std'] = np.std(latent_samples, axis=0)\n",
    "        \n",
    "        print(f\"üìà Latent bounds analysis:\")\n",
    "        print(f\"   Global min: {self.bounds['min'].min():.4f}\")\n",
    "        print(f\"   Global max: {self.bounds['max'].max():.4f}\")\n",
    "        print(f\"   Mean range: [{self.bounds['mean'].min():.4f}, {self.bounds['mean'].max():.4f}]\")\n",
    "        print(f\"   Std range: [{self.bounds['std'].min():.4f}, {self.bounds['std'].max():.4f}]\")\n",
    "    \n",
    "    def _generate_diverse_data(self, num_samples):\n",
    "        \"\"\"Generate diverse training data for bound analysis\"\"\"\n",
    "        def one_hot_encode(x, num_classes=NUM_CLASSES):\n",
    "            x = np.asarray(x, dtype=np.int64)\n",
    "            x = np.clip(x, 0, num_classes-1)\n",
    "            return np.eye(num_classes)[x]\n",
    "        \n",
    "        data = []\n",
    "        for _ in range(num_samples):\n",
    "            pattern_type = np.random.choice(['linear', 'quadratic', 'sinusoidal', 'random'])\n",
    "            sample = np.zeros(INPUT_DIM, dtype=np.int64)\n",
    "            \n",
    "            if pattern_type == 'linear':\n",
    "                base = np.random.randint(0, 5)\n",
    "                for i in range(INPUT_DIM):\n",
    "                    sample[i] = (base + i // 500) % NUM_CLASSES\n",
    "            elif pattern_type == 'quadratic':\n",
    "                base = np.random.randint(0, 5)\n",
    "                for i in range(INPUT_DIM):\n",
    "                    sample[i] = (base + (i // 200) ** 2) % NUM_CLASSES\n",
    "            elif pattern_type == 'sinusoidal':\n",
    "                freq = np.random.uniform(0.001, 0.01)\n",
    "                phase = np.random.uniform(0, 2*np.pi)\n",
    "                for i in range(INPUT_DIM):\n",
    "                    sample[i] = int((np.sin(freq * i + phase) + 1) * NUM_CLASSES // 2) % NUM_CLASSES\n",
    "            else:\n",
    "                sample = np.random.randint(0, NUM_CLASSES, INPUT_DIM)\n",
    "                for i in range(1, INPUT_DIM-1):\n",
    "                    if np.random.random() < 0.3:\n",
    "                        sample[i] = sample[i-1]\n",
    "            \n",
    "            data.append(one_hot_encode(sample))\n",
    "        \n",
    "        return np.array(data)\n",
    "    \n",
    "    def get_sampling_bounds(self):\n",
    "        \"\"\"Get bounds for NSGA2 sampling\"\"\"\n",
    "        # Use mean ¬± 2*std as bounds, but ensure we don't go beyond observed min/max\n",
    "        lower_bounds = np.maximum(\n",
    "            self.bounds['mean'] - 2 * self.bounds['std'],\n",
    "            self.bounds['min']\n",
    "        ).flatten()\n",
    "        \n",
    "        upper_bounds = np.minimum(\n",
    "            self.bounds['mean'] + 2 * self.bounds['std'],\n",
    "            self.bounds['max']\n",
    "        ).flatten()\n",
    "        \n",
    "        return lower_bounds, upper_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc1b1e-45b9-4231-bd97-e8708b2c8488",
   "metadata": {},
   "source": [
    "# Utility function for QOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e8eb46-d5ba-42f8-9b3a-8210cf1f895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================== UTILITIES ===================================================\n",
    "\n",
    "def save_array_configs(solutions, generation_idx):\n",
    "    \"\"\"Save array configurations as .dat files for remote evaluation\"\"\"\n",
    "    gen_dir = f\"./gen/{generation_idx}\"\n",
    "    os.makedirs(gen_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"üíæ Saving {len(solutions)} array configurations for generation {generation_idx}...\")\n",
    "    \n",
    "    for i, solution in enumerate(solutions):\n",
    "        config_num = i + 1  # 1-indexed\n",
    "        filename = f\"{gen_dir}/array_config_{config_num}.dat\"\n",
    "        \n",
    "        try:\n",
    "            # Reshape flat latent vector back to (64, 10)\n",
    "            latent_shaped = solution.reshape(LATENT_DIM, NUM_CATEGORIES)\n",
    "            \n",
    "            # Normalize to valid probability distribution\n",
    "            latent_shaped = latent_shaped - latent_shaped.min(axis=1, keepdims=True)\n",
    "            latent_shaped = latent_shaped / (latent_shaped.sum(axis=1, keepdims=True) + 1e-8)\n",
    "            \n",
    "            # Convert to tensor and decode using the global decoder\n",
    "            with torch.no_grad():\n",
    "                z_tensor = torch.tensor(latent_shaped, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "                decoded = global_decoder(z_tensor)\n",
    "                \n",
    "                # Convert to discrete values (argmax over categories)\n",
    "                array_4096 = torch.argmax(decoded, dim=-1).squeeze().cpu().numpy()\n",
    "            \n",
    "            # Save as .dat file\n",
    "            np.savetxt(filename, array_4096, fmt='%d')\n",
    "            print(f\"   ‚úÖ Saved: {filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error saving {filename}: {e}\")\n",
    "            # Save dummy array on error\n",
    "            dummy_array = np.random.randint(0, NUM_CLASSES, INPUT_DIM)\n",
    "            np.savetxt(filename, dummy_array, fmt='%d')\n",
    "\n",
    "\n",
    "def generate_generation_plots(metrics, generation_idx):\n",
    "    \"\"\"Generate 6 separate plots for each generation\"\"\"\n",
    "    plots_dir = f\"./gen/{generation_idx}/plots\"\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"üìà Generating 6 separate plots for generation {generation_idx}...\")\n",
    "    \n",
    "    # Extract metrics\n",
    "    power = metrics[:, 0]\n",
    "    area = metrics[:, 1]\n",
    "    delay = metrics[:, 2]\n",
    "    \n",
    "    # Plot 1: Power values\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars1 = plt.bar(range(1, 15), power, color='red', alpha=0.7)\n",
    "    plt.title(f'Power Values - Generation {generation_idx}', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Solution Number', fontsize=14)\n",
    "    plt.ylabel('Power', fontsize=14)\n",
    "    plt.xticks(range(1, 15))\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars1):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                f'{power[i]:.1f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_dir}/generation_{generation_idx}_power.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 2: Area values\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars2 = plt.bar(range(1, 15), area, color='blue', alpha=0.7)\n",
    "    plt.title(f'Area Values - Generation {generation_idx}', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Solution Number', fontsize=14)\n",
    "    plt.ylabel('Area', fontsize=14)\n",
    "    plt.xticks(range(1, 15))\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars2):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                f'{area[i]:.1f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_dir}/generation_{generation_idx}_area.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 3: Delay values\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars3 = plt.bar(range(1, 15), delay, color='green', alpha=0.7)\n",
    "    plt.title(f'Delay Values - Generation {generation_idx}', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Solution Number', fontsize=14)\n",
    "    plt.ylabel('Delay', fontsize=14)\n",
    "    plt.xticks(range(1, 15))\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars3):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                f'{delay[i]:.1f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_dir}/generation_{generation_idx}_delay.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 4: Power vs Area\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter1 = plt.scatter(power, area, c=range(14), cmap='viridis', s=150, alpha=0.8, edgecolors='black', linewidth=1)\n",
    "    plt.title(f'Power vs Area - Generation {generation_idx}', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Power', fontsize=14)\n",
    "    plt.ylabel('Area', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter1, label='Solution Number')\n",
    "    # Add solution numbers as labels\n",
    "    for i in range(14):\n",
    "        plt.annotate(f'{i+1}', (power[i], area[i]), xytext=(8, 8), \n",
    "                    textcoords='offset points', fontsize=12, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_dir}/generation_{generation_idx}_power_vs_area.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 5: Power vs Delay\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter2 = plt.scatter(power, delay, c=range(14), cmap='plasma', s=150, alpha=0.8, edgecolors='black', linewidth=1)\n",
    "    plt.title(f'Power vs Delay - Generation {generation_idx}', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Power', fontsize=14)\n",
    "    plt.ylabel('Delay', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter2, label='Solution Number')\n",
    "    # Add solution numbers as labels\n",
    "    for i in range(14):\n",
    "        plt.annotate(f'{i+1}', (power[i], delay[i]), xytext=(8, 8), \n",
    "                    textcoords='offset points', fontsize=12, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_dir}/generation_{generation_idx}_power_vs_delay.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 6: Area vs Delay\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter3 = plt.scatter(area, delay, c=range(14), cmap='coolwarm', s=150, alpha=0.8, edgecolors='black', linewidth=1)\n",
    "    plt.title(f'Area vs Delay - Generation {generation_idx}', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Area', fontsize=14)\n",
    "    plt.ylabel('Delay', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter3, label='Solution Number')\n",
    "    # Add solution numbers as labels\n",
    "    for i in range(14):\n",
    "        plt.annotate(f'{i+1}', (area[i], delay[i]), xytext=(8, 8), \n",
    "                    textcoords='offset points', fontsize=12, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_dir}/generation_{generation_idx}_area_vs_delay.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"   ‚úÖ 6 separate plots saved:\")\n",
    "    print(f\"      - {plots_dir}/generation_{generation_idx}_power.png\")\n",
    "    print(f\"      - {plots_dir}/generation_{generation_idx}_area.png\")\n",
    "    print(f\"      - {plots_dir}/generation_{generation_idx}_delay.png\")\n",
    "    print(f\"      - {plots_dir}/generation_{generation_idx}_power_vs_area.png\")\n",
    "    print(f\"      - {plots_dir}/generation_{generation_idx}_power_vs_delay.png\")\n",
    "    print(f\"      - {plots_dir}/generation_{generation_idx}_area_vs_delay.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd7d9e-c6dc-4767-8055-bb0c72a7ef4d",
   "metadata": {},
   "source": [
    "# Solution Evalution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10a1d39-ae53-4efd-8dd0-914f649f9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ EVALUATING METRICS FUNCTION ===================\n",
    "def read_metrics_from_csv(generation_idx):\n",
    "    \"\"\"Read power, area, delay metrics from CSV files\"\"\"\n",
    "    csv_dir = f\"./gen/{generation_idx}/logs/csv_files\"\n",
    "    \n",
    "    print(f\"üìä Reading metrics from {csv_dir}...\")\n",
    "    \n",
    "    metrics = []\n",
    "    for config_num in range(1, 15):  # 1 to 14\n",
    "        csv_file = f\"{csv_dir}/{config_num}.csv\"\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(csv_file):\n",
    "                # Read CSV file\n",
    "                import pandas as pd\n",
    "                df = pd.read_csv(csv_file)\n",
    "                \n",
    "                # Extract power, area, delay (assuming these are column names)\n",
    "                # Adjust column names based on your actual CSV format\n",
    "                power = float(df['power'].iloc[0]) if 'power' in df.columns else float(df.iloc[0, 0])\n",
    "                area = float(df['area'].iloc[0]) if 'area' in df.columns else float(df.iloc[0, 1])\n",
    "                delay = float(df['delay'].iloc[0]) if 'delay' in df.columns else float(df.iloc[0, 2])\n",
    "                \n",
    "                print(f\"   Config {config_num}: P={power:.2f}, A={area:.2f}, D={delay:.2f}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"   ‚ùå Missing: {csv_file}, using default values\")\n",
    "                power, area, delay = 100.0, 200.0, 50.0  # Default values\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error reading {csv_file}: {e}\")\n",
    "            power, area, delay = 100.0, 200.0, 50.0  # Default values\n",
    "        \n",
    "        # Generate artificial SSIM and PSNR (same as before)\n",
    "        np.random.seed(int((power + area + delay) * 1000) % 2**32)\n",
    "        ssim_loss = 0.1 + np.random.exponential(0.05)\n",
    "        psnr_loss = 5 + np.random.exponential(2)\n",
    "        \n",
    "        metrics.append([power, area, delay, ssim_loss, psnr_loss])\n",
    "    \n",
    "    return np.array(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d3bd1-428f-4ad3-85d6-c41ccf92d1e5",
   "metadata": {},
   "source": [
    "# NSGA LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068771dc-ef1f-47e9-922e-4f62500cb76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ NSGA2 CALLBACK ===================\n",
    "class VerboseCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.generation = 0\n",
    "    \n",
    "    def __call__(self, algorithm):\n",
    "        F = algorithm.pop.get('F')\n",
    "        self.data.append(F)\n",
    "        \n",
    "        print(f\"\\nüìä Generation {self.generation + 1}/{GENERATIONS} Results:\")\n",
    "        print(f\"   Population size: {len(F)}\")\n",
    "        print(f\"   Objective ranges:\")\n",
    "        for i in range(F.shape[1]):\n",
    "            obj_name = ['Power', 'Area', 'Delay', 'SSIM_Loss', 'PSNR_Loss'][i]\n",
    "            print(f\"     {obj_name}: [{F[:, i].min():.3f}, {F[:, i].max():.3f}]\")\n",
    "        \n",
    "        # Find best solutions for each objective\n",
    "        best_indices = np.argmin(F, axis=0)\n",
    "        print(f\"   Best solutions:\")\n",
    "        for i, obj_name in enumerate(['Power', 'Area', 'Delay', 'SSIM_Loss', 'PSNR_Loss']):\n",
    "            best_idx = best_indices[i]\n",
    "            best_val = F[best_idx, i]\n",
    "            print(f\"     {obj_name}: Solution {best_idx+1} = {best_val:.3f}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# ================ NSGA2 PROBLEM CLASS ===================\n",
    "class VAELatentOptimizationProblem(Problem):\n",
    "    def __init__(self, decoder, bounds_analyzer, **kwargs):\n",
    "        self.decoder = decoder\n",
    "        self.bounds_analyzer = bounds_analyzer\n",
    "        self.current_generation = 0\n",
    "        \n",
    "        # Get bounds for the latent space (64 * 10 = 640 variables)\n",
    "        self.xl, self.xu = bounds_analyzer.get_sampling_bounds()\n",
    "        \n",
    "        print(f\"üéØ Problem setup:\")\n",
    "        print(f\"   Variables: {len(self.xl)} (64 dims √ó 10 categories)\")\n",
    "        print(f\"   Objectives: 5 (power, area, delay, ssim_loss, psnr_loss)\")\n",
    "        print(f\"   Bounds: [{self.xl.min():.4f}, {self.xu.max():.4f}]\")\n",
    "        \n",
    "        super().__init__(\n",
    "            n_var=len(self.xl),  # 64 * 10 = 640\n",
    "            n_obj=5,  # power, area, delay, ssim_loss, psnr_loss\n",
    "            xl=self.xl,\n",
    "            xu=self.xu,\n",
    "            elementwise_evaluation=False,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        self.evaluation_count = 0\n",
    "    \n",
    "    def _evaluate(self, X, out, *args, **kwargs):\n",
    "        \"\"\"Evaluate population using remote servers\"\"\"\n",
    "        print(f\"\\nüîÑ Evaluating Generation {self.current_generation + 1} with {len(X)} solutions...\")\n",
    "        \n",
    "        # Save array configurations as .dat files\n",
    "        save_array_configs(X, self.current_generation + 1)\n",
    "        \n",
    "        print(f\"üöÄ Starting remote evaluation (this will take ~3 hours)...\")\n",
    "        run_remote_compute_cluster_automation(self.current_generation + 1)\n",
    "        \n",
    "        # Read metrics from CSV files\n",
    "        metrics = read_metrics_from_csv(self.current_generation + 1)\n",
    "        \n",
    "        # Generate plots for this generation\n",
    "        generate_generation_plots(metrics, self.current_generation + 1)\n",
    "        \n",
    "        # Set objectives in the output dictionary\n",
    "        out[\"F\"] = metrics\n",
    "        \n",
    "        print(f\"‚úÖ Generation {self.current_generation + 1} evaluation complete!\")\n",
    "        self.evaluation_count += len(X)\n",
    "        self.current_generation += 1\n",
    "    \n",
    "    def evaluate_single(self, latent_flat, thread_id):\n",
    "        \"\"\"Evaluate a single solution\"\"\"\n",
    "        try:\n",
    "            # Reshape flat latent vector back to (64, 10)\n",
    "            latent_shaped = latent_flat.reshape(LATENT_DIM, NUM_CATEGORIES)\n",
    "            \n",
    "            # Normalize to valid probability distribution (ensure each row sums to 1)\n",
    "            latent_shaped = latent_shaped - latent_shaped.min(axis=1, keepdims=True)  # Make positive\n",
    "            latent_shaped = latent_shaped / (latent_shaped.sum(axis=1, keepdims=True) + 1e-8)  # Normalize\n",
    "            \n",
    "            # Convert to tensor and decode\n",
    "            with torch.no_grad():\n",
    "                z_tensor = torch.tensor(latent_shaped, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "                decoded = self.decoder(z_tensor)\n",
    "                \n",
    "                # Convert to discrete values (argmax over categories)\n",
    "                array_4096 = torch.argmax(decoded, dim=-1).squeeze().cpu().numpy()\n",
    "            \n",
    "            # Get metrics\n",
    "            power, area, delay, ssim_loss, psnr_loss = get_metrics(array_4096)\n",
    "            \n",
    "            print(f\"   Solution {thread_id}: P={power:.2f}, A={area:.2f}, D={delay:.2f}, S={ssim_loss:.3f}, PSNR={psnr_loss:.2f}\")\n",
    "            \n",
    "            return [power, area, delay, ssim_loss, psnr_loss]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in solution {thread_id}: {e}\")\n",
    "            return [1000, 1000, 1000, 10, 100]  # Penalty values\n",
    "    \n",
    "    def close_pool(self):\n",
    "        \"\"\"Close the thread pool\"\"\"\n",
    "        self.pool.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ================ MAIN OPTIMIZATION FUNCTION ===================\n",
    "def run_vae_nsga2_optimization():\n",
    "    \"\"\"Main function to run NSGA2 optimization in VAE latent space with remote evaluation\"\"\"\n",
    "    global global_decoder\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"üöÄ NSGA2 VAE Latent Space Optimization with Remote Evaluation\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load the VAE model\n",
    "    print(\"üì¶ Loading VAE model...\")\n",
    "    encoder, decoder, checkpoint = load_model_fixed(LATENT_DIM)\n",
    "    global_decoder = decoder  # Set global decoder\n",
    "    \n",
    "    # Analyze latent space bounds\n",
    "    print(\"üîç Analyzing latent space...\")\n",
    "    bounds_analyzer = LatentSpaceBounds(encoder, decoder, num_samples=500)\n",
    "    \n",
    "    # Create NSGA2 problem\n",
    "    print(\"üéØ Setting up NSGA2 problem...\")\n",
    "    problem = VAELatentOptimizationProblem(decoder, bounds_analyzer)\n",
    "    \n",
    "    # Setup callback\n",
    "    callback = VerboseCallback()\n",
    "    \n",
    "    # Configure NSGA2 algorithm\n",
    "    algorithm = NSGA2(\n",
    "        pop_size=POPULATION,\n",
    "        sampling=FloatRandomSampling(),\n",
    "        crossover=SBX(prob=0.9, eta=15),\n",
    "        mutation=PolynomialMutation(prob=1.0/problem.n_var, eta=20),\n",
    "    )\n",
    "    \n",
    "    # Setup termination\n",
    "    termination = get_termination(\"n_gen\", GENERATIONS)\n",
    "    \n",
    "    print(\"üèÉ‚Äç‚ôÇÔ∏è Starting optimization...\")\n",
    "    print(f\"   Algorithm: NSGA2\")\n",
    "    print(f\"   Population: {POPULATION}\")\n",
    "    print(f\"   Generations: {GENERATIONS}\")\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: Each generation takes ~3 hours!\")\n",
    "    print(f\"   ‚è∞ Total estimated time: {GENERATIONS * 3} hours\")\n",
    "    \n",
    "    # Run optimization\n",
    "    result = minimize(\n",
    "        problem,\n",
    "        algorithm,\n",
    "        termination,\n",
    "        callback=callback,\n",
    "        seed=SEED,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üéâ Optimization Complete!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"üìä Final Results:\")\n",
    "    print(f\"   Solutions found: {len(result.F)}\")\n",
    "    print(f\"   Objective ranges:\")\n",
    "    for i, obj_name in enumerate(['Power', 'Area', 'Delay', 'SSIM_Loss', 'PSNR_Loss']):\n",
    "        print(f\"     {obj_name}: [{result.F[:, i].min():.3f}, {result.F[:, i].max():.3f}]\")\n",
    "    \n",
    "    # Save results\n",
    "    save_results(result, callback)\n",
    "    \n",
    "    # Plot convergence across all generations\n",
    "    plot_convergence(callback.data)\n",
    "    \n",
    "    return result, callback\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ================ RESULTS HANDLING ===================\n",
    "def save_results(result, callback):\n",
    "    \"\"\"Save optimization results\"\"\"\n",
    "    results_dir = \"nsga2_vae_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Save final solutions\n",
    "    np.save(f\"{results_dir}/final_solutions_X.npy\", result.X)\n",
    "    np.save(f\"{results_dir}/final_solutions_F.npy\", result.F)\n",
    "    \n",
    "    # Save convergence data\n",
    "    convergence_data = np.array(callback.data)\n",
    "    np.save(f\"{results_dir}/convergence_data.npy\", convergence_data)\n",
    "    \n",
    "    # Save summary\n",
    "    summary = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'latent_dim': LATENT_DIM,\n",
    "        'generations': GENERATIONS,\n",
    "        'population': POPULATION,\n",
    "        'final_solutions_count': len(result.F),\n",
    "        'objective_names': ['Power', 'Area', 'Delay', 'SSIM_Loss', 'PSNR_Loss'],\n",
    "        'objective_ranges': {\n",
    "            f'obj_{i}': [float(result.F[:, i].min()), float(result.F[:, i].max())]\n",
    "            for i in range(result.F.shape[1])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(f\"{results_dir}/summary.json\", 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Results saved to {results_dir}/\")\n",
    "\n",
    "def plot_convergence(convergence_data):\n",
    "    \"\"\"Plot convergence of objectives over generations\"\"\"\n",
    "    data = np.array(convergence_data)  # Shape: (generations, population, objectives)\n",
    "    \n",
    "    obj_names = ['Power', 'Area', 'Delay', 'SSIM Loss', 'PSNR Loss']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for obj_idx in range(5):\n",
    "        ax = axes[obj_idx]\n",
    "        \n",
    "        # Extract min, mean, max for each generation\n",
    "        obj_data = data[:, :, obj_idx]  # Shape: (generations, population)\n",
    "        \n",
    "        generations = range(1, len(obj_data) + 1)\n",
    "        min_vals = np.min(obj_data, axis=1)\n",
    "        mean_vals = np.mean(obj_data, axis=1)\n",
    "        max_vals = np.max(obj_data, axis=1)\n",
    "        \n",
    "        # Plot\n",
    "        ax.plot(generations, min_vals, 'g-', label='Best', linewidth=2)\n",
    "        ax.plot(generations, mean_vals, 'b--', label='Mean', linewidth=1)\n",
    "        ax.plot(generations, max_vals, 'r:', label='Worst', linewidth=1)\n",
    "        ax.fill_between(generations, min_vals, max_vals, alpha=0.2)\n",
    "        \n",
    "        ax.set_title(f'{obj_names[obj_idx]} Convergence')\n",
    "        ax.set_xlabel('Generation')\n",
    "        ax.set_ylabel(obj_names[obj_idx])\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove empty subplot\n",
    "    axes[-1].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('nsga2_vae_results/convergence_plot.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìà Convergence plot saved to nsga2_vae_results/convergence_plot.png\")\n",
    "\n",
    "# ================ RUN OPTIMIZATION ===================\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"üöÄ Starting NSGA2 optimization in {LATENT_DIM}D latent space\")\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"Generations: {GENERATIONS}, Population: {POPULATION}\")\n",
    "    result, callback = run_vae_nsga2_optimization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
